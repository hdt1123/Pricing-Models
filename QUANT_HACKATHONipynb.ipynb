{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QUANT HACKATHONipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3_cemnD9Zh6"
      },
      "source": [
        "######  Note: No training but all codes work perfectly fine as the kernel keeps on being disconnected and was told by Manas to submit as it is for now without training. \n",
        "###### If you have any questions, feel free to contact us and we will answer it as soon as possible. Thanks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGReHA1E9cAm"
      },
      "source": [
        "#### Two notebooks have been submitted for Beat the Speed Pricing Models Competition. This notebook uses Stacking regression (consists of lgbm and xgboost) to train on the dataset. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x83CZR2z9eCu"
      },
      "source": [
        "### Explainability \n",
        "Light GBM, XGBoost, and Stack Ensemble have been used to predict the \"val_lvsvcharge\". \n",
        "- Each model used only a few parameters to increase its accuracy while preventing overfitting. \n",
        "- Light GBM and XGBoost are tree structures so it's easy to understand, while Stack Ensemble combines the two alogrithms \n",
        "- Models have smooth sensitivities on the inputs. (does not have jumps of results with a small change of variable)\n",
        "- Models have clear comments and explanations (see the upcoming codes)\n",
        "- Models which training is fully explained and transparent "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C-Wsh849gXU"
      },
      "source": [
        "### Importing Libaries and dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmPjlQto9VQq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "961f53e3-ba7d-41da-e777-922d4fc6bde4"
      },
      "source": [
        "import alphien\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn import linear_model\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.svm import SVR\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "from lightgbm import LGBMRegressor\n",
        "import lightgbm as lgb\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-62bcfd2dbae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0malphien\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'alphien'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiATXX_p9XmI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "88ea71ea-a04e-4a45-903c-2b215719bc72"
      },
      "source": [
        "pip install mlxtend"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mlxtend in /usr/local/lib/python3.6/dist-packages (0.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from mlxtend) (50.3.0)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.1.2)\n",
            "Requirement already satisfied: matplotlib>=1.5.1 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (3.2.2)\n",
            "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from mlxtend) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1->mlxtend) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=1.5.1->mlxtend) (2.4.7)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18->mlxtend) (0.16.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.17.1->mlxtend) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoRZGrVi909M"
      },
      "source": [
        "### Preprocessing and Feature Extraction "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhKXqipT95oU"
      },
      "source": [
        "#The section where raw dataset (as provided by Alphien) gets cleaned and transformed into the input \n",
        "#format the model expects. Raw data (alphien.data.DataLoader) is input. The columns with object dtype\n",
        "#are removed. The size of the combination of training and testing is set in this function. \n",
        "#Splitting feature/label and training/test. \n",
        "import re\n",
        "def data_processing (raw):\n",
        "    dl = raw\n",
        "    dataGen = dl.batch(fromRow=1, toRow=500000)\n",
        "    data = next(dataGen)\n",
        "    data = data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "    data = data[:150000]\n",
        "    print(data.shape)\n",
        "    data = data.select_dtypes(exclude=['object'])\n",
        "    y = data['val_lvsvcharge'].reset_index(drop=True)\n",
        "    X = data.loc[:, data.columns != 'val_lvsvcharge']\n",
        "    y = data.iloc[:,-1:]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kmguMwA96Ys"
      },
      "source": [
        "X_train, X_test, y_train, y_test = data_processing(alphien.data.DataLoader()) #initializing training and testing"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EkbAOMN94Mc"
      },
      "source": [
        "print(X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPMRRo1A-Nvo"
      },
      "source": [
        "#define mean squre error and max absolute error for prediction\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from sklearn.metrics import max_error, mean_absolute_error\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "def mse(y, y_pred):\n",
        "    return mean_squared_error(y, y_pred)\n",
        "\n",
        "def cv_rmse(model):\n",
        "    testing = -cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv=kfolds)\n",
        "    print(testing)\n",
        "    rmse = np.sqrt(testing)\n",
        "    return (rmse)\n",
        "\n",
        "def mae(y, y_pred):\n",
        "    return np.abs(max_error(y, y_pred))\n",
        "\n",
        "\n",
        "def cv_rmax(model):\n",
        "    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_max_error\", cv=kfolds))\n",
        "    return (rmse)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oa_5lotq-Pk9"
      },
      "source": [
        "#plot feature importance for a specific model and return important features for feature selection\n",
        "def plotImp(model, total_features):\n",
        "    features_used = []\n",
        "    features_used_amt = []\n",
        "    feature_list = list(model.feature_importances_)\n",
        "    feature_len = len(feature_list)\n",
        "    for i in range(feature_len):\n",
        "        if feature_list[i] > 0:\n",
        "            features_used.append(total_features[i])\n",
        "            features_used_amt.append(feature_list[i])\n",
        "    print(features_used)\n",
        "    sns.barplot(y=list(features_used), x=list(features_used_amt))\n",
        "    return features_used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPU1HnvU-k5y"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95eXr_rK-mb_"
      },
      "source": [
        "### LightGBM\n",
        "To achieve a higher accuracy in the lightgbm model, we have included the following parameters in this model:\n",
        "- num_leaves\n",
        "    - the larger the num_leaves, the higher accuracy it is, but may cause over-fitting\n",
        "- max_bin\n",
        "    - the larger the max_bin value, the higher the accuracy but the slower it is \n",
        "- learning_rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORWzOiWD-jo2"
      },
      "source": [
        "#initialize lightGBM model and train it as preliminary experiment for feature selection\n",
        "from math import sqrt\n",
        "lgb_model = LGBMRegressor(objective='regression',num_leaves=35, \n",
        "                          learning_rate=0.05, n_estimators=300)\n",
        "lgb_model.fit(X_train, y_train)\n",
        "y_lgb_pred = lgb_model.predict(X_test)\n",
        "LGB_model_error = mse(y_test, y_lgb_pred)\n",
        "print(f'LGBM Mean Squared Error - {LGB_model_error}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeyNYRZO-ogN"
      },
      "source": [
        "#plot feature importance and extract important features\n",
        "total_features = list(X_train)\n",
        "# plotImp(lgb_model, total_features)\n",
        "L_column_selected = plotImp(lgb_model, total_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9S9vf-Q_-9-J"
      },
      "source": [
        "### XGBoost\n",
        "To achieve a higher accuracy in XGBRegressor, we have included the following parameters in this model:\n",
        "- learning_rate\n",
        "- n_estimators\n",
        "- max_depth \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjOd0kaT-sLa"
      },
      "source": [
        "#initialize XGBoost model and train it as preliminary experiment for feature selection\n",
        "XGB_model = XGBRegressor(learning_rate=0.03,n_estimators=500, \n",
        "                         max_depth=6, objective='reg:squarederror')\n",
        "XGB_model.fit(X_train, y_train)\n",
        "y_XGB_predict = XGB_model.predict(X_test)\n",
        "XGB_model_error = mse(y_test, y_XGB_predict)\n",
        "print(f'XGBoost Mean Squared Error - {XGB_model_error}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mU172FHC_BGo"
      },
      "source": [
        "#plot feature importance and extract important features\n",
        "# plotImp(XGB_model, total_features)\n",
        "X_column_selected = plotImp(XGB_model, total_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTY9FR6v_Edr"
      },
      "source": [
        "#### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iAfOlzK_CcJ"
      },
      "source": [
        "#outer join important features obtained from two pretrained models\n",
        "select=[f for f in X_column_selected or L_column_selected]\n",
        "select_X_train=X_train[X_train.columns.intersection(select)]\n",
        "select_X_test=X_test[X_test.columns.intersection(select)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uecRwM1N_GpW"
      },
      "source": [
        "print(select_X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mn7LRoBx_LXW"
      },
      "source": [
        "### Stacking Regression\n",
        "Stacking regression is used to combine lgb_model and XGB_model via a meta-regressor\n",
        "- lower variance and lower bias \n",
        "- have higher accuracy \n",
        "- lowering the possibility of overfitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWJLRDas_HoG"
      },
      "source": [
        "# Training\n",
        "# Note: this block of code has the feature similar to Training Loop\n",
        "#       Please run the previous blocks starting from the sectionï¼š Model in order to run this block of codes \n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from numpy import sort\n",
        "from mlxtend.regressor import StackingCVRegressor\n",
        "stack_model = StackingCVRegressor(regressors=(lgb_model, XGB_model),\n",
        "                                meta_regressor=XGB_model,\n",
        "                                use_features_in_secondary=True)\n",
        "\n",
        "stack_model.fit(np.array(select_X_train), np.array(y_train))\n",
        "y_stack_pred = stack_model.predict(np.array(select_X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS2HL9ue_Njy"
      },
      "source": [
        "print('MSE score on testing data:')\n",
        "print(mse(y_test, y_stack_pred))\n",
        "print('MAE score on testing data:')\n",
        "print(mae(y_test, y_stack_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noa_WcLF_QyR"
      },
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzCIXrNp_ObC"
      },
      "source": [
        "# remove the target column and remove parameters that are 'object' type\n",
        "def predDataTransform(unknown_data, unknown_select):\n",
        "    unknown_data = unknown_data.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
        "    unknown_data = unknown_data.select_dtypes(exclude=['object'])\n",
        "    unknown_data = unknown_data[unknown_data.columns.intersection(unknown_select)]\n",
        "    return unknown_data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTCu4UKS_SMW"
      },
      "source": [
        "#suppose you have unseen data\n",
        "# Change the amoung of rows that you want to predict in the second row of this block of cells \n",
        "dl = alphien.data.DataLoader()\n",
        "unseen = dl.batch(fromRow=1, toRow=10000)\n",
        "data = next(unseen)\n",
        "data = data.iloc[:,:-1]\n",
        "selected_data = predDataTransform(data, select)\n",
        "def myPredictFunc(newData, model):\n",
        "    return model.predict(np.array(selected_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2svp83W_TGk"
      },
      "source": [
        "# stack_model is used for prediction function\n",
        "ypred = myPredictFunc(selected_data, stack_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqFBTBC-_i1N"
      },
      "source": [
        "print(ypred[:10])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}